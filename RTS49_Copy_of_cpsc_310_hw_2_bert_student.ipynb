{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redorhcs/CPSC-310-HW2/blob/main/RTS49_Copy_of_cpsc_310_hw_2_bert_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CPSC 310 Homework 2 - BERT Sentiment Analysis\n",
        "\n",
        "The purpose of this portion of the assignment is to apply the BERT text classification model to the bulletin board you have been working on.\n",
        "\n",
        "The following Colab notebook will take you through the process of producing a sentiment analysis model trained on a dataset of posts. We will be using the Bidirectional Encoder Representations from Transformers (BERT) model and finetuning it for our data. Before starting the assignment, read about the model [here](https://arxiv.org/pdf/1810.04805.pdf). We will adapt the model to classify between positive and negative posts.\n",
        "\n",
        "As you work through the notebook, there will be portions that you must fill out to move on to the next step. Instructions will be provided for each specific task, but your submitted code should contain code filled out for each section. Submit your modified version of the notebook as part of your assignment."
      ],
      "metadata": {
        "id": "Md8SzcHiY-P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Pre-Processing"
      ],
      "metadata": {
        "id": "WzeK2uFYaB6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import required libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#transformers\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TFBertModel\n",
        "\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "\n",
        "#keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#set seed for reproducibility\n",
        "seed=42\n",
        "\n",
        "#set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.despine()\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)"
      ],
      "metadata": {
        "id": "WFofEiIQwF8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO\n",
        "\n",
        "Before writing this step, upload data.csv and test_data.csv to the Colab environment. To do this, click the folder icon on the left-hand side of the notebook and drag/drop or upload both files."
      ],
      "metadata": {
        "id": "DYcucoiDSViN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: load the training and testing dataset into a dataframe called train_df and test_df, respectively. Use the same headers as in the csv (name, post, sentiment)"
      ],
      "metadata": {
        "id": "7dfByOVuwoSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lemmatization and Pre-Processing\n",
        "\n",
        "In the following blocks, we provide the code to preprocess the posts. We do this to standardize the post formats to make it easier for our classifier to recognize components such as usernames, URLs, and emojis. We also clean up other aspects of posts that make it difficult for them to be processed.\n",
        "\n",
        "A portion of this `preprocess`  function includes a Lemmatizer, which you can learn more about [here](https://en.wikipedia.org/wiki/Lemmatisation)."
      ],
      "metadata": {
        "id": "9J2yLob_ajvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining dictionary containing all emojis with their meanings.\n",
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
        "\n",
        "## Defining set containing all stopwords in english.\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
      ],
      "metadata": {
        "id": "FMJb3fDgyUyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "def preprocess(textdata):\n",
        "    processedText = []\n",
        "    \n",
        "    # Create Lemmatizer and Stemmer.\n",
        "    wordLemm = WordNetLemmatizer()\n",
        "    \n",
        "    # Defining regex patterns.\n",
        "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    userPattern       = '@[^\\s]+'\n",
        "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "    \n",
        "    for post in textdata:\n",
        "        post = post.lower()\n",
        "        \n",
        "        # Replace all URls with 'URL'\n",
        "        post = re.sub(urlPattern,' URL',post)\n",
        "        # Replace all emojis.\n",
        "        for emoji in emojis.keys():\n",
        "            post = post.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
        "        # Replace @USERNAME to 'USER'.\n",
        "        post = re.sub(userPattern,' USER', post)        \n",
        "        # Replace all non alphabets.\n",
        "        post = re.sub(alphaPattern, \" \", post)\n",
        "        # Replace 3 or more consecutive letters by 2 letter.\n",
        "        post = re.sub(sequencePattern, seqReplacePattern, post)\n",
        "\n",
        "        postwords = ''\n",
        "        for word in post.split():\n",
        "            # Checking if the word is a stopword.\n",
        "            #if word not in stopwordlist:\n",
        "            if len(word)>1:\n",
        "                # Lemmatizing the word.\n",
        "                word = wordLemm.lemmatize(word)\n",
        "                postwords += (word+' ')\n",
        "            \n",
        "        processedText.append(postwords)\n",
        "        \n",
        "    return processedText"
      ],
      "metadata": {
        "id": "vo_GRb92xPWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting up posts and sentiments for processing\n",
        "posts, sentiments = list(train_df['post']), list(train_df['sentiment'])\n",
        "test_posts, test_sentiments = list(test_df['post']), list(test_df['sentiment'])\n"
      ],
      "metadata": {
        "id": "hdKlWsEjxZl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Run the preprocessing step on posts and test_posts, putting the processed text into two variables named processedtext and test_processedtext, respectively."
      ],
      "metadata": {
        "id": "8sRy6Lz7jrTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data Organization"
      ],
      "metadata": {
        "id": "iFiptPSybd5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split our training dataset into train and validation.\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_df['post'].values, train_df['sentiment'].values, test_size=0.1, stratify=train_df['sentiment'].values, random_state=seed)\n",
        "X_test, y_test = test_df['post'].values, test_df['sentiment'].values\n",
        "\n",
        "print(f'Data Split done.')"
      ],
      "metadata": {
        "id": "wCFAgNlGypkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d14862-c439-4b6b-a47d-d133070e3bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Split done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### One Hot Encoding \n",
        "Learn about this step [here](https://https://en.wikipedia.org/wiki/One-hot#Natural_language_processing)."
      ],
      "metadata": {
        "id": "wKg_I6rlbj9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_le = y_train.copy()\n",
        "y_valid_le = y_valid.copy()\n",
        "y_test_le = y_test.copy()\n"
      ],
      "metadata": {
        "id": "bagG2pTb1MK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe = preprocessing.OneHotEncoder()\n",
        "y_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
        "y_valid = ohe.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\n",
        "y_test = ohe.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "4qL0iZ6d1hvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"TRAINING DATA: {X_train.shape[0]}\\nVALIDATION DATA: {X_valid.shape[0]}\\nTESTING DATA: {X_test.shape[0]}\" )"
      ],
      "metadata": {
        "id": "qoA3uN6x1kF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7f9e3d-5682-4a56-f8b4-5444dbc39166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING DATA: 900\n",
            "VALIDATION DATA: 100\n",
            "TESTING DATA: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tokenization\n",
        "\n",
        "Learn about this step [here](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html).\n",
        "\n",
        "You may notice the setup of an `attention_masks` variable below. The purpose of this variable is to standardize the length of each post (as some post could be only a few characters while others could be much longer). This ensures that we can pass in the data with one format."
      ],
      "metadata": {
        "id": "b2nrq6kNcAbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "d3vR4_fz12M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN=128\n",
        "def tokenize(data,max_len=MAX_LEN) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for i in range(len(data)):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            data[i],\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)"
      ],
      "metadata": {
        "id": "paITSQXk2DDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_attention_masks = tokenize(X_train, MAX_LEN)\n",
        "val_input_ids, val_attention_masks = tokenize(X_valid, MAX_LEN)\n",
        "test_input_ids, test_attention_masks = tokenize(X_test, MAX_LEN)"
      ],
      "metadata": {
        "id": "N_CJ6SvH1-W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Setup"
      ],
      "metadata": {
        "id": "YIsUtvXCcinM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "bfvy66pQy2ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### End Layer Addition\n",
        "\n",
        "In the following step, we take the pretrained model and add our own layers to tune the BERT algorithm to our use case. The key layer to understand here is the output layer, which takes the output of our model and maps it to a probability of 2 outputs, either a 0 (negative) or 1 (positive)."
      ],
      "metadata": {
        "id": "_dTpuoH6cmUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_model(bert_model, max_len=MAX_LEN):\n",
        "    \n",
        "    ##params###\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "    accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "\n",
        "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    \n",
        "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    \n",
        "    embeddings = bert_model([input_ids,attention_masks])[1]\n",
        "    \n",
        "    output = tf.keras.layers.Dense(2, activation=\"softmax\")(embeddings)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n",
        "    \n",
        "    model.compile(opt, loss=loss, metrics=accuracy)\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "JQtCR6X6zIW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model Creation"
      ],
      "metadata": {
        "id": "aX0YBtfuc9FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(bert_model, MAX_LEN)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "86iG97392RBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model Training\n",
        "\n",
        "Running the code in this section will train the model on our data. Before you run this step, make sure that your runtime in Colab has a GPU assigned to it. If you would like to check that it does, run the code block containing `!nvidia-smi` below to see if there is an available GPU. If there is no GPU, follow the tutorial [here](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm) to get it setup. You should not have to pay to add a GPU.\n",
        "\n",
        "This step will take a while (up to 20 minutes), so don't be worried if it is running slower than expected. "
      ],
      "metadata": {
        "id": "XaNzHmE9dAn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_bert = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "Ktcl22Ur2Vaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Use and Evaluation"
      ],
      "metadata": {
        "id": "L8ZbyJGLdx4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the prediction on our testing dataset\n",
        "result_bert = model.predict([test_input_ids,test_attention_masks])\n"
      ],
      "metadata": {
        "id": "AHgq9mvX8XIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f5916c-edba-40c3-ca34-fab84ff37149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 172ms/step\n",
            "['\"Decrease in crime and increase in public safety under Trentino\\'s strong law enforcement policies.\"', '\"Trentino\\'s leadership successful in improving standard of living for citizens through job creation and economic growth.\"', '\"Trentino\\'s government improves access to education and healthcare for all citizens, regardless of their income.\"', '\"Trentino\\'s government improves the lives of veterans and elderly by providing support and benefits.\"', '\"Sylvanian cuisine has a wide range of delicious and traditional dishes.\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Using the predictions stored in result_bert, write a Python script to export a csv containing only posts in the testing set our model deems to be positive. Save this csv and upload it as part of your submission."
      ],
      "metadata": {
        "id": "57GtF6YikURy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "14I5nJvC8X-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize our accuracy on the testing set. \n",
        "\n",
        "y_pred_bert =  np.zeros_like(result_bert)\n",
        "y_pred_bert[np.arange(len(y_pred_bert)), result_bert.argmax(1)] = 1\n",
        "\n",
        "def conf_matrix(y, y_pred, title):\n",
        "    fig, ax =plt.subplots(figsize=(5,5))\n",
        "    labels=['Negative', 'Positive']\n",
        "    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n",
        "    plt.title(title, fontsize=20)\n",
        "    ax.xaxis.set_ticklabels(labels, fontsize=17) \n",
        "    ax.yaxis.set_ticklabels(labels, fontsize=17)\n",
        "    ax.set_ylabel('Test', fontsize=20)\n",
        "    ax.set_xlabel('Predicted', fontsize=20)\n",
        "    plt.show()\n",
        "conf_matrix(y_test.argmax(1), y_pred_bert.argmax(1),'BERT Sentiment Analysis\\nConfusion Matrix')\n",
        "print('\\tClassification Report for BERT:\\n\\n',classification_report(y_test,y_pred_bert, target_names=['Negative','Positive']))\n",
        "\n",
        "\n",
        "#TODO: Screenshot the output of this code block and upload it as part of your submission"
      ],
      "metadata": {
        "id": "1x_bB7fl8tAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}